{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e61460fc-8568-49c9-b59c-56eab25902e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration Template \n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\"fs.azure.account.oauth2.client.id\": \"[client_id]\",\n",
    "\"fs.azure.account.oauth2.client.secret\": \"[secret_id]\",\n",
    "\"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/[tenent_id]/oauth2/token\"}\n",
    "\n",
    "# Mounting point \n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://[container-name]@[storage account name].dfs.core.windows.net\", \n",
    "mount_point = \"/mnt/[some name you want to mount it as]\",  #i called it \"homerental\"\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264a51e7-d197-48fc-a034-7fe8eb86a301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8252555143920091>, line 10\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m configs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.auth.type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOAuth\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth.provider.type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc4ee8c3d-51ed-471f-bcd5-7e8f5c3887a2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.secret\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO2a8Q~zvbfkOI_P-w80fClelB8S9m8zP3aQAaa~U\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.endpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://login.microsoftonline.com/a7f389b2-923e-49d4-bfd2-46d74c68dadb/oauth2/token\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Mounting point \u001B[39;00m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# storage account name: homerentalanalysis, container name: home-rental-data\u001B[39;00m\n",
       "\u001B[0;32m---> 10\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(\n",
       "\u001B[1;32m     11\u001B[0m source \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://home-rental-data@homerentalanalysis.dfs.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n",
       "\u001B[1;32m     12\u001B[0m mount_point \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/homerental\u001B[39m\u001B[38;5;124m\"\u001B[39m,   \u001B[38;5;66;03m#name that I call the mounting point \u001B[39;00m\n",
       "\u001B[1;32m     13\u001B[0m extra_configs \u001B[38;5;241m=\u001B[39m configs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:172\u001B[0m, in \u001B[0;36mprettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    171\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o448.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/homerental; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/homerental\n",
       "\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:161)\n",
       "\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1437)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1463)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:94)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:94)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:94)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:94)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:159)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1457)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/homerental\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1110)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1110)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21(DbfsServerBackend.scala:357)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21$adapted(DbfsServerBackend.scala:356)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$legacyWithRPCContext$1(UnaryRpcHandler.scala:512)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n",
       "\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n",
       "\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n",
       "\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n",
       "\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)\n",
       "\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n",
       "\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)\n",
       "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
       "\tat scala.util.Success.map(Try.scala:213)\n",
       "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
       "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n",
       "\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n",
       "\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n",
       "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
       "\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
       "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-8252555143920091>, line 10\u001B[0m\n\u001B[1;32m      2\u001B[0m configs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.auth.type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOAuth\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth.provider.type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc4ee8c3d-51ed-471f-bcd5-7e8f5c3887a2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.secret\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO2a8Q~zvbfkOI_P-w80fClelB8S9m8zP3aQAaa~U\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.oauth2.client.endpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://login.microsoftonline.com/a7f389b2-923e-49d4-bfd2-46d74c68dadb/oauth2/token\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Mounting point \u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# storage account name: homerentalanalysis, container name: home-rental-data\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(\n\u001B[1;32m     11\u001B[0m source \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://home-rental-data@homerentalanalysis.dfs.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n\u001B[1;32m     12\u001B[0m mount_point \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/homerental\u001B[39m\u001B[38;5;124m\"\u001B[39m,   \u001B[38;5;66;03m#name that I call the mounting point \u001B[39;00m\n\u001B[1;32m     13\u001B[0m extra_configs \u001B[38;5;241m=\u001B[39m configs)\n\nFile \u001B[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:172\u001B[0m, in \u001B[0;36mprettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    170\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    171\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o448.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/homerental; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/homerental\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:161)\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1437)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1463)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:94)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:94)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:94)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:94)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:159)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1457)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/homerental\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1110)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1110)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21(DbfsServerBackend.scala:357)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21$adapted(DbfsServerBackend.scala:356)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$legacyWithRPCContext$1(UnaryRpcHandler.scala:512)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n",
       "errorSummary": "<span class='ansi-red-fg'>ExecutionError</span>: An error occurred while calling o448.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/homerental; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/homerental\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:161)\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1437)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1463)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:94)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:94)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:94)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:94)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:159)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1457)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/homerental\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1110)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1110)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21(DbfsServerBackend.scala:357)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21$adapted(DbfsServerBackend.scala:356)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$legacyWithRPCContext$1(UnaryRpcHandler.scala:512)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleJettyRpc(UnaryRpcHandler.scala:88)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleJettyRpcWithAggregatedContent(UnaryRpcService.scala:522)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$2(UnaryRpcService.scala:433)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.withAttributionContext(UnaryRpcService.scala:187)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleContextAwareJettyRpcWithAggregatedContent$1(UnaryRpcService.scala:428)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.handleContextAwareJettyRpcWithAggregatedContent(UnaryRpcService.scala:427)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$2(UnaryRpcService.scala:407)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:108)\n\tat com.databricks.rpc.armeria.UnaryRpcServiceInternal.$anonfun$handleJettyJsonRpc$1(UnaryRpcService.scala:402)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e937e1-1552-47ec-8091-609732bc8d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mountPoint</th><th>source</th><th>encryptionType</th></tr></thead><tbody><tr><td>/databricks-datasets</td><td>databricks-datasets</td><td></td></tr><tr><td>/Volumes</td><td>UnityCatalogVolumes</td><td></td></tr><tr><td>/databricks/mlflow-tracking</td><td>databricks/mlflow-tracking</td><td></td></tr><tr><td>/databricks-results</td><td>databricks-results</td><td></td></tr><tr><td>/databricks/mlflow-registry</td><td>databricks/mlflow-registry</td><td></td></tr><tr><td>/mnt/homerental</td><td>abfss://home-rental-data@homerentalanalysis.dfs.core.windows.net</td><td></td></tr><tr><td>/Volume</td><td>DbfsReserved</td><td></td></tr><tr><td>/volumes</td><td>DbfsReserved</td><td></td></tr><tr><td>/</td><td>DatabricksRoot</td><td></td></tr><tr><td>/volume</td><td>DbfsReserved</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "/databricks-datasets",
         "databricks-datasets",
         ""
        ],
        [
         "/Volumes",
         "UnityCatalogVolumes",
         ""
        ],
        [
         "/databricks/mlflow-tracking",
         "databricks/mlflow-tracking",
         ""
        ],
        [
         "/databricks-results",
         "databricks-results",
         ""
        ],
        [
         "/databricks/mlflow-registry",
         "databricks/mlflow-registry",
         ""
        ],
        [
         "/mnt/homerental",
         "abfss://home-rental-data@homerentalanalysis.dfs.core.windows.net",
         ""
        ],
        [
         "/Volume",
         "DbfsReserved",
         ""
        ],
        [
         "/volumes",
         "DbfsReserved",
         ""
        ],
        [
         "/",
         "DatabricksRoot",
         ""
        ],
        [
         "/volume",
         "DbfsReserved",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "mountPoint",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "encryptionType",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check if the entire data lake is mounted to this location\n",
    "display(dbutils.fs.mounts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979d7688-b326-46f0-84d0-bfa84bfb8526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4409944105541500#setting/sparkui/1102-052241-nsjf9flw/driver-3743606872629057115\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f440ddd6a80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in Databricks, you don't have to create a Spark session from scatch \n",
    "# such as from pyspark.sql import SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1011c879-f0bf-44b9-8fbc-51d41f007104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write Spark code to read files\n",
    "capacity = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/homerental/raw-data/capacity.csv\")\n",
    "pricing = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/homerental/raw-data/pricing.csv\")\n",
    "applicant = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/mnt/homerental/raw-data/applicant.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82041685-f411-48b9-900c-083d94a02038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------+\n| Full building name|Coded name|Capacity|\n+-------------------+----------+--------+\n| Amazing Apartment |     APT-1|     200|\n|Brilliant Apartment|     APT-3|     300|\n|Fantastic Apartment|     APT-5|     400|\n|      Great College| College-1|     300|\n| Incredible College| College-3|     500|\n|  Wonderful College| College-5|     500|\n+-------------------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# display table output\n",
    "capacity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "233e2d77-e9dc-4d68-b5e0-bbfc06f78192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+-----------------+------------------+\n|Room Building|  Room Type|Monthly Rent|Monthly Utilities|Total Monthly Cost|\n+-------------+-----------+------------+-----------------+------------------+\n|        APT-1|Single Room|        1000|               50|              1050|\n|        APT-1|  1 Bedroom|         950|               50|              1000|\n|        APT-1|  2 Bedroom|         900|               50|               950|\n|        APT-1|  5 Bedroom|         800|               50|               850|\n|        APT-3|Single Room|        2000|              100|              2100|\n|        APT-3|  1 Bedroom|        1500|              100|              1600|\n|        APT-3|  2 Bedroom|        1000|              100|              1100|\n|        APT-3|  5 Bedroom|         900|              100|              1000|\n|        APT-5|Single Room|        1500|              120|              1620|\n|        APT-5|  1 Bedroom|        1200|              120|              1320|\n|        APT-5|  2 Bedroom|        1050|              120|              1170|\n|        APT-5|  5 Bedroom|        1000|              120|              1120|\n|    College-1|Single Room|        1000|               50|              1050|\n|    College-1|  1 Bedroom|         950|               50|              1000|\n|    College-1|  2 Bedroom|         850|               50|               900|\n|    College-1|  5 Bedroom|         800|               50|               850|\n|    College-3|Single Room|        1350|              100|              1450|\n|    College-3|  1 Bedroom|        1200|              100|              1300|\n|    College-3|  2 Bedroom|        1000|              100|              1100|\n|    College-3|  5 Bedroom|         850|              100|               950|\n+-------------+-----------+------------+-----------------+------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# display table output\n",
    "pricing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7e9e702-e414-4596-8c3a-954448e2c2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------------+-------------+--------------+------------+----------------+-------------------+--------------------------+----------------+---------------+----------------------+---------------------+--------------------+-------------------+-----------------+---------------------------------+------------------+----------------+--------------------+--------------------+--------------------+-------------+\n| Age|Entry ID|           Gender|         City|State Province|Zip Postcode|Application Date|Application Status |Classification Description|Offer Reply Date|Offer Sent Date|Apply for Scholarship?|Children accompanied?|Partner accompanied?|   Location offered|Room Type offered|Aboriginal/Torres Strait Islander|Course Description|Enrollment Class| 1st Room Preference| 2nd Room Preference| 3rd Room Preference| Student Type|\n+----+--------+-----------------+-------------+--------------+------------+----------------+-------------------+--------------------------+----------------+---------------+----------------------+---------------------+--------------------+-------------------+-----------------+---------------------------------+------------------+----------------+--------------------+--------------------+--------------------+-------------+\n|17.0|  400001|             Male|       City 1|    Province 1|      1234.0|       11/1/2024|     Offer Accepted|                      New |        11/21/24|       11/20/24|                  True|                False|               False|  Amazing Apartment|      Single Room|                              No |         Program 1|   Undergraduate|Single Room - Ama...|2-Bedroom - Brill...|5-Bedroom - Brill...|     Domestic|\n|23.0|  400002|           Female|International| International|        NULL|        11/16/24|     Offer Declined|                  Returner|        11/21/24|       11/20/24|                 False|                False|               False|Brilliant Apartment|        1-Bedroom|                              No |         Program 2|    Postgraduate|2-Bedroom - Great...|Single Room - Ama...|Single Room - Fan...|International|\n|20.0|  400003|Prefer not to say|       City 3|   Province 10|      2300.0|         11/3/23|     Offer Accepted|                  Returner|        11/24/24|       11/20/24|                 False|                False|               False|Fantastic Apartment|       2-Bedroom |                             NULL|         Program 3|             PhD|1-Bedroom - Incre...|2-Bedroom - Incre...|Single Room - Ama...|     Domestic|\n|20.0|  400004|            Other|International| International|        NULL|         10/5/24|     Offer Declined|                      New |        11/30/24|       11/20/24|                  True|                False|               False|      Great College|        5-Bedroom|                              No |         Program 3|   Undergraduate|Single Room - Ama...|Single Room - Fan...|1-Bedroom - Incre...|International|\n|22.0|  400005|             Male|       City 5|    Province 1|        NULL|         01/5/23|     Offer Accepted|                  Returner|         1/24/24|        1/20/24|                 False|                False|               False|Brilliant Apartment|      Single Room|                              No |         Program 1|    Postgraduate|2-Bedroom - Wonde...|2-Bedroom - Wonde...|Single Room - Fan...|     Domestic|\n|20.0|  400006|           Female|International| International|      1234.0|        02/02/23|     Offer Declined|                      New |        02/13/24|       02/10/24|                  True|                False|               False|  Wonderful College|        5-Bedroom|                              No |         Program 2|    Postgraduate|2-Bedroom - Brill...|2-Bedroom - Amazi...|1-Bedroom - Wonde...|International|\n|23.0|  400007|           Female|International| International|      2000.0|        03/03/24|     Offer Declined|                      New |         03/1/24|       03/10/24|                  True|                False|               False|  Wonderful College|        5-Bedroom|                              No |         Program 3|    Postgraduate| Single Room - APT 1| One Bedroom - APT 3|Single Room - Fan...|International|\n|25.0|  400008|             Male|       City 8|    Province 2|        NULL|        04/04/24|     Offer Accepted|                      New |        03/14/24|       03/10/24|                 False|                 True|                True|Fantastic Apartment|      Single Room|                              No |         Program 1|    Postgraduate|Single Room - Fan...|Single Room - Bri...|2-Bedroom - Incre...|     Domestic|\n|21.0|  400009|             Male|       City 9|    Province 9|        NULL|        05/05/24|     Offer Accepted|                      New |         5/30/24|       05/10/24|                 False|                False|               False|      Great College|        5-Bedroom|                              No |         Program 2|   Undergraduate|1-Bedroom - Amazi...|Single Room - Bri...|5-Bedroom - Brill...|     Domestic|\n|20.0|  400010|Prefer not to say|International| International|      1234.0|        06/06/23|     Offer Accepted|                  Returner|         6/19/24|       06/10/24|                 False|                False|               False|Brilliant Apartment|        5-Bedroom|                              No |         Program 1|             PhD|Single Room - Ama...|1-Bedroom - Fanta...|Single Room - Fan...|International|\n|19.0|  400011|           Female|International| International|        NULL|        07/07/23|     Offer Accepted|                  Returner|         7/13/24|       07/10/24|                 False|                False|                True|Fantastic Apartment|      Single Room|                              No |         Program 2|             PhD|5-Bedroom - Brill...|5-Bedroom - Amazi...|5-Bedroom - Fanta...|International|\n|21.0|  400012|           Female|      City 12|    Province 3|      2100.0|        08/08/24|     Offer Accepted|                  Returner|         8/20/24|       08/15/24|                 False|                False|               False|Fantastic Apartment|      Single Room|                              No |         Program 2|   Undergraduate|Single Room - Ama...|Single Room - Bri...|Single Room - Fan...|     Domestic|\n|22.0|  400013|Prefer not to say|International| International|        NULL|        09/09/23|     Offer Accepted|                  Returner|            NULL|           NULL|                  True|                 True|                True|Brilliant Apartment|       2-Bedroom |                              No |         Program 1|             PhD|5-Bedroom - Fanta...|5-Bedroom - Amazi...|Single Room - Won...|International|\n|25.0|  400014|Prefer not to say|      City 14|    Province 3|      1234.0|        10/10/24|     Offer Accepted|                  Returner|         9/21/24|       09/20/24|                 False|                 True|               False|Fantastic Apartment|        1-Bedroom|                              No |         Program 3|   Undergraduate|1-Bedroom - Fanta...|Single Room - Ama...|Single Room - Fan...|     Domestic|\n|20.0|  400015|             Male|      City 15| International|      1234.0|        10/20/23|     Offer Accepted|                      New |        11/10/23|       11/10/23|                  True|                 True|                True|Fantastic Apartment|        1-Bedroom|                              No |         Program 2|             PhD|2-Bedroom - Fanta...|5-Bedroom - Brill...|1-Bedroom - Fanta...|International|\n|20.0|  400016|             Male|      City 16|    Province 3|        NULL|        10/29/23|     Offer Declined|                      New |        11/17/23|        11/9/23|                  True|                False|               False|  Wonderful College|        5-Bedroom|                              Yes|         Program 1|   Undergraduate|1-Bedroom - Great...|Single Room - Bri...|Single Room - Fan...|     Domestic|\n|21.0|  400017|Prefer not to say|International| International|        NULL|        11/25/23|     Offer Accepted|                      New |        12/12/23|       12/10/23|                  True|                False|               False|  Amazing Apartment|        1-Bedroom|                              No |         Program 1|   Undergraduate|Single Room - Ama...|1-Bedroom - Amazi...|Single Room - Fan...|International|\n|18.0|  400018|             Male|      City 18|    Province 9|      1234.0|        11/30/24|     Offer Accepted|                  Returner|        12/14/23|       12/10/23|                  True|                False|               False|      Great College|       2-Bedroom |                              No |         Program 1|             PhD|2-Bedroom - Amazi...|1-Bedroom - Brill...|2-Bedroom - Fanta...|     Domestic|\n|21.0|  400019|            Other|International| International|        NULL|        11/01/23|     Offer Declined|                      New |        11/20/23|       11/10/23|                 False|                False|               False|  Wonderful College|        5-Bedroom|                              No |         Program 1|   Undergraduate|Single Room - Ama...|Single Room - Ama...|5-Bedroom - Fanta...|International|\n|24.0|  400020|Prefer not to say|      City 20|    Province 9|        NULL|        12/12/24|     Offer Accepted|                      New |            NULL|           NULL|                  True|                False|               False|  Amazing Apartment|        1-Bedroom|                              No |         Program 1|    Postgraduate|1-Bedroom - Amazi...|2-Bedroom - Amazi...|Single Room - Fan...|     Domestic|\n+----+--------+-----------------+-------------+--------------+------------+----------------+-------------------+--------------------------+----------------+---------------+----------------------+---------------------+--------------------+-------------------+-----------------+---------------------------------+------------------+----------------+--------------------+--------------------+--------------------+-------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# display table output\n",
    "applicant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b91d9a-abf4-48b9-8819-2eabd20ff262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Full building name: string (nullable = true)\n |-- Coded name: string (nullable = true)\n |-- Capacity: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# printSchema checks the data type of each column \n",
    "# Helpful to check if anything is wrong in the data frame, like wrong data type \n",
    "# Do this check for each data frame \n",
    "capacity.printSchema()\n",
    "\n",
    "# \"Capacity\" column should be integer instead of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2ef421-a81f-4819-acb0-8cdc4fef7856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Room Building: string (nullable = true)\n |-- Room Type: string (nullable = true)\n |-- Monthly Rent: string (nullable = true)\n |-- Monthly Utilities: string (nullable = true)\n |-- Total Monthly Cost: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "pricing.printSchema()\n",
    "\n",
    "# \"Monthly Rent\", \"Monthly Utilities\", and \"Total Monthly Cost\" columns should be integer instead of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1995a417-e972-417b-9b24-2c8f55682f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Age: string (nullable = true)\n |-- Entry ID: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State Province: string (nullable = true)\n |-- Zip Postcode: string (nullable = true)\n |-- Application Date: string (nullable = true)\n |-- Application Status : string (nullable = true)\n |-- Classification Description: string (nullable = true)\n |-- Offer Reply Date: string (nullable = true)\n |-- Offer Sent Date: string (nullable = true)\n |-- Apply for Scholarship?: string (nullable = true)\n |-- Children accompanied?: string (nullable = true)\n |-- Partner accompanied?: string (nullable = true)\n |-- Location offered: string (nullable = true)\n |-- Room Type offered: string (nullable = true)\n |-- Aboriginal/Torres Strait Islander: string (nullable = true)\n |-- Course Description: string (nullable = true)\n |-- Enrollment Class: string (nullable = true)\n |-- 1st Room Preference: string (nullable = true)\n |-- 2nd Room Preference: string (nullable = true)\n |-- 3rd Room Preference: string (nullable = true)\n |-- Student Type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "applicant.printSchema()\n",
    "\n",
    "# \"Age\", \"Entry ID\" columns should be integer instead of string \n",
    "# \"Application Date\", \"Offer Reply Date\", \"Offer Sent Date\" columns should be date instead of string\n",
    "# \"Apply for Scholarship?\"\", \"Children accompanied?\"\", \"Partner accompanied?\"\" columns should be bool instead of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b10338bb-3cde-4fdd-a5d7-72db56c4562a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Change data type method 1: using withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555320c7-d1bd-459c-a56f-38e3b96195f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col \n",
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4063d58-b5c9-40f0-a681-6e982b949a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Change data type from string to integer \n",
    "# Create a new column in the right data format that is overwriting the old column in the wrong data format \n",
    "\n",
    "# From capacity data frame \n",
    "# \"Capacity\" column should be integer instead of string\n",
    "capacity = capacity.withColumn(\"Capacity\", col(\"Capacity\").cast(IntegerType()))\n",
    "\n",
    "# From pricing data frame \n",
    "# \"Monthly Rent\", \"Monthly Utilities\", and \"Total Monthly Cost\" columns should be integer instead of string.\n",
    "pricing = pricing.withColumn(\"Monthly Rent\", col(\"Monthly Rent\").cast(IntegerType()))\\\n",
    "    .withColumn(\"Monthly Utilities\", col(\"Monthly Utilities\").cast(IntegerType()))\\\n",
    "    .withColumn(\"Total Monthly Cost\", col(\"Total Monthly Cost\").cast(IntegerType()))\n",
    "\n",
    "# From applicant data frame \n",
    "# \"Age\", \"Entry ID\" columns should be integer instead of string \n",
    "applicant = applicant.withColumn(\"Age\", col(\"Age\").cast(IntegerType()))\\\n",
    "    .withColumn(\"Entry ID\", col(\"Entry ID\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da3a6bd-2ab1-430c-a589-ad79fd666401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Full building name: string (nullable = true)\n |-- Coded name: string (nullable = true)\n |-- Capacity: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# print schema to confirm successful data type switch \n",
    "capacity.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1752815-da07-4bf8-b413-749e51324f2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Change data type method 2: using inferSchema during the file reading step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312a8bbe-329e-4443-bb9c-4320af0c8e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "applicant = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(\"/mnt/homerental/raw-data/applicant.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa516fb6-1c1e-40bd-843d-dc7357f2a180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Age: double (nullable = true)\n |-- Entry ID: integer (nullable = true)\n |-- Gender: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State Province: string (nullable = true)\n |-- Zip Postcode: double (nullable = true)\n |-- Application Date: string (nullable = true)\n |-- Application Status : string (nullable = true)\n |-- Classification Description: string (nullable = true)\n |-- Offer Reply Date: date (nullable = true)\n |-- Offer Sent Date: date (nullable = true)\n |-- Apply for Scholarship?: boolean (nullable = true)\n |-- Children accompanied?: boolean (nullable = true)\n |-- Partner accompanied?: boolean (nullable = true)\n |-- Location offered: string (nullable = true)\n |-- Room Type offered: string (nullable = true)\n |-- Aboriginal/Torres Strait Islander: string (nullable = true)\n |-- Course Description: string (nullable = true)\n |-- Enrollment Class: string (nullable = true)\n |-- 1st Room Preference: string (nullable = true)\n |-- 2nd Room Preference: string (nullable = true)\n |-- 3rd Room Preference: string (nullable = true)\n |-- Student Type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "applicant.printSchema()\n",
    "\n",
    "# most columns are correctly interpreted as boolean and date data types, except \"Application Date\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b01c14-e77a-454b-8035-42d75d12f448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# From applicant data frame \n",
    "# \"Age\", \"Entry ID\" columns should be integer instead of string \n",
    "applicant = applicant.withColumn(\"Application Date\", col(\"Application Date\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b9f997-3756-4e20-b5f2-e8e6f2c0cfeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Age: double (nullable = true)\n |-- Entry ID: integer (nullable = true)\n |-- Gender: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State Province: string (nullable = true)\n |-- Zip Postcode: double (nullable = true)\n |-- Application Date: date (nullable = true)\n |-- Application Status : string (nullable = true)\n |-- Classification Description: string (nullable = true)\n |-- Offer Reply Date: date (nullable = true)\n |-- Offer Sent Date: date (nullable = true)\n |-- Apply for Scholarship?: boolean (nullable = true)\n |-- Children accompanied?: boolean (nullable = true)\n |-- Partner accompanied?: boolean (nullable = true)\n |-- Location offered: string (nullable = true)\n |-- Room Type offered: string (nullable = true)\n |-- Aboriginal/Torres Strait Islander: string (nullable = true)\n |-- Course Description: string (nullable = true)\n |-- Enrollment Class: string (nullable = true)\n |-- 1st Room Preference: string (nullable = true)\n |-- 2nd Room Preference: string (nullable = true)\n |-- 3rd Room Preference: string (nullable = true)\n |-- Student Type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "applicant.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef7aaeb5-f6fe-400f-b034-fb0635d2b11c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ede26edd-10cf-4618-9928-36027de378c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+-----------------+------------------+\n|Room Building|  Room Type|Monthly Rent|Monthly Utilities|Total Monthly Cost|\n+-------------+-----------+------------+-----------------+------------------+\n|        APT-5|  2 Bedroom|        1050|              120|              1170|\n|    College-5|Single Room|        1300|              120|              1420|\n|        APT-5|  5 Bedroom|        1000|              120|              1120|\n|        APT-5|Single Room|        1500|              120|              1620|\n|        APT-5|  1 Bedroom|        1200|              120|              1320|\n|    College-5|  1 Bedroom|        1200|              120|              1320|\n|    College-5|  5 Bedroom|        1000|              120|              1120|\n|    College-5|  2 Bedroom|        1050|              120|              1170|\n|        APT-3|  2 Bedroom|        1000|              100|              1100|\n|        APT-3|Single Room|        2000|              100|              2100|\n|    College-3|  2 Bedroom|        1000|              100|              1100|\n|    College-3|Single Room|        1350|              100|              1450|\n|        APT-3|  5 Bedroom|         900|              100|              1000|\n|    College-3|  5 Bedroom|         850|              100|               950|\n|        APT-3|  1 Bedroom|        1500|              100|              1600|\n|    College-3|  1 Bedroom|        1200|              100|              1300|\n|    College-1|Single Room|        1000|               50|              1050|\n|        APT-1|  1 Bedroom|         950|               50|              1000|\n|        APT-1|  5 Bedroom|         800|               50|               850|\n|    College-1|  1 Bedroom|         950|               50|              1000|\n+-------------+-----------+------------+-----------------+------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Question 1. Find the housing option with the highest monthly utilities\n",
    "# If you want to show all columns in the table \n",
    "highest_utilities_housing = pricing.orderBy(\"Monthly Utilities\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bfdc34e-cdf6-4e04-b3d5-05d6eb59f972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------------+\n|Room Building|  Room Type|Monthly Utilities|\n+-------------+-----------+-----------------+\n|        APT-5|  2 Bedroom|              120|\n|    College-5|Single Room|              120|\n|        APT-5|  5 Bedroom|              120|\n|        APT-5|Single Room|              120|\n|        APT-5|  1 Bedroom|              120|\n|    College-5|  1 Bedroom|              120|\n|    College-5|  5 Bedroom|              120|\n|    College-5|  2 Bedroom|              120|\n|        APT-3|  2 Bedroom|              100|\n|        APT-3|Single Room|              100|\n|    College-3|  2 Bedroom|              100|\n|    College-3|Single Room|              100|\n|        APT-3|  5 Bedroom|              100|\n|    College-3|  5 Bedroom|              100|\n|        APT-3|  1 Bedroom|              100|\n|    College-3|  1 Bedroom|              100|\n|    College-1|Single Room|               50|\n|        APT-1|  1 Bedroom|               50|\n|        APT-1|  5 Bedroom|               50|\n|    College-1|  1 Bedroom|               50|\n+-------------+-----------+-----------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# If you want to show only selected columns in the table \n",
    "highest_utilities_housing = pricing.orderBy(\"Monthly Utilities\", ascending=False).select(\"Room Building\", \"Room Type\", \"Monthly Utilities\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "home rental analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}